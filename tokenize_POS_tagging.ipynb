{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tJxsUVDTzpAaXjBcRisqzsf718gWR9Ip",
      "authorship_tag": "ABX9TyNmpANrY9WNAeIPu/YlNBCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nuwantha97/Sinhala_spell_and_grammer_checker/blob/Notebooks/tokenize_POS_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sinling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKDg6e36AEKX",
        "outputId": "312c3ab4-6aa2-4b17-ff90-6aa5c03d9d92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sinling\n",
            "  Downloading sinling-0.3.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting emoji (from sinling)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sinling) (1.4.2)\n",
            "Collecting pygtrie (from sinling)\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting sklearn-crfsuite (from sinling)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sinling) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sinling) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->sinling) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->sinling) (4.67.1)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->sinling)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->sinling) (1.6.0)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->sinling) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (3.5.0)\n",
            "Downloading sinling-0.3.6-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygtrie, python-crfsuite, emoji, sklearn-crfsuite, sinling\n",
            "Successfully installed emoji-2.14.0 pygtrie-2.5.0 python-crfsuite-0.9.11 sinling-0.3.6 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uyzHRz_yAj-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf6b7976-4e8e-45ee-b1e4-2c8b93ab13cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Tuple, Set, Dict\n",
        "import logging\n",
        "import math\n",
        "import json\n",
        "\n",
        "class SinhalaPOSTagger:\n",
        "    \"\"\"A Part-of-Speech tagger for Sinhala using the Viterbi algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the Sinhala POS tagger.\"\"\"\n",
        "        self.unknown_prob = math.log(1e-10)\n",
        "        self.bigram_cnt: Dict[Tuple[str, str], int] = defaultdict(int)\n",
        "        self.unigram_cnt: Dict[str, int] = defaultdict(int)\n",
        "        self.tag_count: Dict[str, int] = defaultdict(int)\n",
        "        self.tag_word_count: Counter = Counter()\n",
        "        self.transition_probabilities: Dict[Tuple[str, str], float] = defaultdict(lambda: self.unknown_prob)\n",
        "        self.emission_probabilities: Dict[Tuple[str, str], float] = defaultdict(lambda: self.unknown_prob)\n",
        "        self.states: Set[str] = set()\n",
        "\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def ngrams(self, text: List[str], n: int) -> List[Tuple[str, ...]]:\n",
        "        \"\"\"Generate n-grams from text.\"\"\"\n",
        "        return [tuple(text[i:i + n]) for i in range(len(text) - n + 1)]\n",
        "\n",
        "    def clean_sinhala(self, word: str) -> str:\n",
        "        \"\"\"Clean Sinhala word and handle Unicode normalization.\n",
        "\n",
        "        Args:\n",
        "            word: Input Sinhala word\n",
        "\n",
        "        Returns:\n",
        "            Cleaned word\n",
        "        \"\"\"\n",
        "        # Remove any whitespace\n",
        "        word = re.sub(r'\\s+', '', word)\n",
        "        # Normalize Zero Width Joiner and Zero Width Non-Joiner\n",
        "        word = re.sub(r'[\\u200D\\u200C]', '', word)\n",
        "        return word\n",
        "\n",
        "    def load_corpus(self, corpus_file: str) -> List[List[Tuple[str, str]]]:\n",
        "        \"\"\"Load Sinhala tagged corpus from file.\n",
        "\n",
        "        Expected format (JSON):\n",
        "        [\n",
        "            [[\"word1\", \"tag1\"], [\"word2\", \"tag2\"]],  # Sentence 1\n",
        "            [[\"word3\", \"tag3\"], [\"word4\", \"tag4\"]]   # Sentence 2\n",
        "        ]\n",
        "        \"\"\"\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            corpus = json.load(f)\n",
        "        return corpus\n",
        "\n",
        "    def train(self, corpus_file: str) -> None:\n",
        "        \"\"\"Train the POS tagger on Sinhala corpus.\n",
        "\n",
        "        Args:\n",
        "            corpus_file: Path to the JSON file containing tagged Sinhala corpus\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Starting training process...\")\n",
        "\n",
        "        corpus = self.load_corpus(corpus_file)\n",
        "        tagged_words = []\n",
        "        all_tags = []\n",
        "\n",
        "        # Process corpus\n",
        "        for sentence in corpus:\n",
        "            all_tags.append(\"START\")\n",
        "            for word, tag in sentence:\n",
        "                if tag and tag not in ['NIL']:\n",
        "                    all_tags.append(tag)\n",
        "                    word = self.clean_sinhala(word)\n",
        "                    tagged_words.append((tag, word))\n",
        "            all_tags.append(\"END\")\n",
        "\n",
        "        # Calculate probabilities\n",
        "        self._calculate_probabilities(tagged_words, all_tags)\n",
        "\n",
        "        self.logger.info(f\"Training complete. Found {len(self.states)} unique tags.\")\n",
        "\n",
        "    def _calculate_probabilities(self, tagged_words: List[Tuple[str, str]], all_tags: List[str]) -> None:\n",
        "        \"\"\"Calculate all probabilities needed for the model.\"\"\"\n",
        "        # Count occurrences\n",
        "        for tag, word in tagged_words:\n",
        "            self.tag_count[tag] += 1\n",
        "            self.tag_word_count[(tag, word)] += 1\n",
        "\n",
        "        # Calculate bigram and unigram counts\n",
        "        for bigram in self.ngrams(all_tags, 2):\n",
        "            self.bigram_cnt[bigram] += 1\n",
        "        for tag in all_tags:\n",
        "            self.unigram_cnt[tag] += 1\n",
        "\n",
        "        # Calculate transition probabilities\n",
        "        for bigram in self.bigram_cnt:\n",
        "            if self.unigram_cnt[bigram[0]] > 0:\n",
        "                prob = self.bigram_cnt[bigram] / self.unigram_cnt[bigram[0]]\n",
        "                self.transition_probabilities[bigram] = math.log(prob) if prob > 0 else self.unknown_prob\n",
        "\n",
        "        # Calculate emission probabilities\n",
        "        for tag, word in tagged_words:\n",
        "            if self.tag_count[tag] > 0:\n",
        "                prob = self.tag_word_count[(tag, word)] / self.tag_count[tag]\n",
        "                self.emission_probabilities[(tag, word)] = math.log(prob) if prob > 0 else self.unknown_prob\n",
        "\n",
        "        # Store states\n",
        "        self.states = set(self.tag_count.keys())\n",
        "\n",
        "    def viterbi(self, observable: List[str], states: Set[str]) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Implement Viterbi algorithm for POS tagging.\"\"\"\n",
        "        if not states:\n",
        "            self.logger.error(\"No states provided for Viterbi algorithm\")\n",
        "            return []\n",
        "\n",
        "        V = [{}]  # Viterbi matrix\n",
        "        path = {}\n",
        "\n",
        "        # Initialize\n",
        "        for state in states:\n",
        "            V[0][state] = (self.transition_probabilities[(\"START\", state)] +\n",
        "                          self.emission_probabilities[(state, observable[0])])\n",
        "            path[state] = [state]\n",
        "\n",
        "        # Run Viterbi\n",
        "        for t in range(1, len(observable)):\n",
        "            V.append({})\n",
        "            newpath = {}\n",
        "\n",
        "            for state in states:\n",
        "                emit_p = self.emission_probabilities[(state, observable[t])]\n",
        "                (prob, state0) = max(\n",
        "                    (V[t-1][y0] + self.transition_probabilities[(y0, state)] + emit_p, y0)\n",
        "                    for y0 in states\n",
        "                )\n",
        "                V[t][state] = prob\n",
        "                newpath[state] = path[state0] + [state]\n",
        "            path = newpath\n",
        "\n",
        "        # Find best path\n",
        "        (prob, state) = max((V[len(observable) - 1][y], y) for y in states)\n",
        "        return list(zip(observable, path[state]))\n",
        "\n",
        "    def tag_sentence(self, sentence: List[str]) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Tag a Sinhala sentence with POS tags.\"\"\"\n",
        "        if not self.states:\n",
        "            self.logger.error(\"Model not trained. Please run train() first.\")\n",
        "            return []\n",
        "\n",
        "        # Tokenize each word in the sentence with error handling\n",
        "        tokenized_words = []\n",
        "        for word in sentence:\n",
        "            if isinstance(word, str):\n",
        "                tokens = tokenizer.tokenize(word)\n",
        "                tokenized_words.append(tokens[0] if tokens else word)\n",
        "            else:\n",
        "                tokenized_words.append(str(word))\n",
        "\n",
        "        cleaned_words = [self.clean_sinhala(w) for w in tokenized_words]\n",
        "        return self.viterbi(cleaned_words, self.states)\n",
        "\n",
        "    def save_model(self, file_path: str) -> None:\n",
        "      \"\"\"Save the trained model to a file.\"\"\"\n",
        "      model_data = {\n",
        "          'bigram_cnt': {\"|\".join(k): v for k, v in self.bigram_cnt.items()},\n",
        "          'unigram_cnt': dict(self.unigram_cnt),\n",
        "          'tag_count': dict(self.tag_count),\n",
        "          'tag_word_count': {\"|\".join(k): v for k, v in self.tag_word_count.items()},\n",
        "          'transition_probabilities': {\"|\".join(k): v for k, v in dict(self.transition_probabilities).items()},\n",
        "          'emission_probabilities': {\"|\".join(k): v for k, v in dict(self.emission_probabilities).items()},\n",
        "          'states': list(self.states)\n",
        "      }\n",
        "\n",
        "      with open(file_path, 'w', encoding='utf-8') as f:\n",
        "          json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def load_model(self, file_path: str) -> None:\n",
        "        \"\"\"Load a trained model from a file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            model_data = json.load(f)\n",
        "\n",
        "        self.bigram_cnt = defaultdict(int, {tuple(k.split(\"|\")): v for k, v in model_data['bigram_cnt'].items()})\n",
        "        self.unigram_cnt = defaultdict(int, model_data['unigram_cnt'])\n",
        "        self.tag_count = defaultdict(int, model_data['tag_count'])\n",
        "        self.tag_word_count = Counter({tuple(k.split(\"|\")): v for k, v in model_data['tag_word_count'].items()})\n",
        "        self.transition_probabilities = defaultdict(\n",
        "            lambda: self.unknown_prob,\n",
        "            {tuple(k.split(\"|\")): v for k, v in model_data['transition_probabilities'].items()}\n",
        "        )\n",
        "        self.emission_probabilities = defaultdict(\n",
        "            lambda: self.unknown_prob,\n",
        "            {tuple(k.split(\"|\")): v for k, v in model_data['emission_probabilities'].items()}\n",
        "        )\n",
        "        self.states = set(model_data['states'])"
      ],
      "metadata": {
        "id": "kbcS3vbpDIWP"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Projects/Sinhala Spell and Grammer checker/POS data/pos_nod.csv\", on_bad_lines='skip', sep=',', engine='python')\n",
        "\n",
        "# Create a temporary file to store the filtered data\n",
        "with open(\"/content/drive/MyDrive/Projects/Sinhala Spell and Grammer checker/POS data/pos_nod.csv\", 'r', encoding='utf-8') as infile, open('temp.csv', 'w', encoding='utf-8', newline='') as outfile:\n",
        "    reader = csv.reader(infile)\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    for row in reader:\n",
        "        if len(row) <= 2:  # Keep rows with 2 or fewer fields\n",
        "            writer.writerow(row)\n",
        "\n",
        "# Load the modified CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('temp.csv')\n",
        "\n",
        "# prompt: add df to word and tag colunm names\n",
        "\n",
        "df.columns = ['word', 'tag']\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ep5_FytbDYTR",
        "outputId": "92847b93-69f5-4101-eef4-5b926eeceb20"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             word  tag\n",
              "0          මිසයිල  NNJ\n",
              "1         ප්‍රහාර  NNC\n",
              "2           වලින්   CM\n",
              "3      පලස්තීනුවෝ  NNP\n",
              "4               4  NUM\n",
              "...           ...  ...\n",
              "31453   වීරඹුගෙදර  NNP\n",
              "31454     පොතුහැර  NNP\n",
              "31455   බංගලාවත්ත  NNP\n",
              "31456       ලතීෆ්  NNP\n",
              "31457     තව්ෆික්  NNP\n",
              "\n",
              "[31458 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-570b870e-e88f-4ee9-92a3-3bf31c472086\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>මිසයිල</td>\n",
              "      <td>NNJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ප්‍රහාර</td>\n",
              "      <td>NNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>වලින්</td>\n",
              "      <td>CM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>පලස්තීනුවෝ</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31453</th>\n",
              "      <td>වීරඹුගෙදර</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31454</th>\n",
              "      <td>පොතුහැර</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31455</th>\n",
              "      <td>බංගලාවත්ත</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31456</th>\n",
              "      <td>ලතීෆ්</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31457</th>\n",
              "      <td>තව්ෆික්</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31458 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-570b870e-e88f-4ee9-92a3-3bf31c472086')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-570b870e-e88f-4ee9-92a3-3bf31c472086 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-570b870e-e88f-4ee9-92a3-3bf31c472086');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b57b49c0-ccda-4e24-9ad0-727a4430be92\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b57b49c0-ccda-4e24-9ad0-727a4430be92')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b57b49c0-ccda-4e24-9ad0-727a4430be92 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_816bf2c1-4267-4c6b-a461-b83b7741b96a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_816bf2c1-4267-4c6b-a461-b83b7741b96a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 31458,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31458,\n        \"samples\": [\n          \"\\u0dbb\\u0dcf\\u0da2\\u0db0\\u0dcf\\u0db1\\u0dd2\\u0dba\\u0da7\",\n          \"\\u0db4\\u0dd0\\u0dad\\u0dd4\\u0db8\\u0dca\",\n          \"\\u0db1\\u0ddc\\u0dc0\\u0dda\\u0daf\\u0dd0\\u0dba\\u0dd2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 35,\n        \"samples\": [\n          \"QUE\",\n          \"NIP\",\n          \"ABB\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: df split to training and test data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'df' is your DataFrame with 'word' and 'tag' columns\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42) # 80% training, 20% testing\n",
        "\n",
        "print(\"Training data size:\", len(train_df))\n",
        "print(\"Testing data size:\", len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3z8MYkxDxRx",
        "outputId": "6a0a87fc-4808-48c1-b0b3-6fe035027aac"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 25166\n",
            "Testing data size: 6292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sinling import SinhalaTokenizer\n",
        "\n",
        "# Create your training data in JSON format\n",
        "tokenizer = SinhalaTokenizer()\n",
        "\n",
        "# Create your training data in JSON format with tokenized words\n",
        "training_data = []\n",
        "for i in range(len(train_df)):\n",
        "    try:\n",
        "        word = train_df.iloc[i]['word']\n",
        "        if not isinstance(word, str):\n",
        "            continue\n",
        "        # Tokenize the Sinhala word and handle empty results\n",
        "        tokens = tokenizer.tokenize(word)\n",
        "        tokenized_word = tokens[0] if tokens else word\n",
        "        training_data.append([[tokenized_word, train_df.iloc[i]['tag']]])\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"Skipping row {i}: {e}\")\n",
        "\n",
        "# Save training data to file\n",
        "with open('sinhala_corpus.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(training_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Create and train the tagger\n",
        "tagger = SinhalaPOSTagger()\n",
        "tagger.train('/content/sinhala_corpus.json')"
      ],
      "metadata": {
        "id": "swfgr2qeD5oH"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "tagger.save_model('sinhala_pos_model.json')"
      ],
      "metadata": {
        "id": "QejVaau2D8vf"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sinhala_word_split(text: str) -> list:\n",
        "    # Split on spaces first\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Handle punctuation marks\n",
        "    final_words = []\n",
        "    for token in tokens:\n",
        "        # If token ends with punctuation, separate it\n",
        "        match = re.match(r'(.*?)([?!,.]*)$', token)\n",
        "        if match:\n",
        "            word, punct = match.groups()\n",
        "            if word:\n",
        "                final_words.append(word)\n",
        "            if punct:\n",
        "                final_words.extend(list(punct))\n",
        "\n",
        "    return final_words\n",
        "\n",
        "# Test the tokenizer\n",
        "text = \"කුරුල්ලා නිවෙස් අත්හැර නොයති\"\n",
        "words = sinhala_word_split(text)"
      ],
      "metadata": {
        "id": "qD__ViB5Lfo-"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sentence = tagger.tag_sentence(words)\n",
        "\n",
        "# Print results\n",
        "for word, tag in tagged_sentence:\n",
        "    print(f\"{word}: {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWRzjZNNEABY",
        "outputId": "18c28dd7-d3ef-455d-8ca9-2f5f52c6bb85"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "කුරුල්ලා: NNC\n",
            "නිවෙස්: NNC\n",
            "අත්හැර: VNF\n",
            "නොයති: VP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: collect tags to array\n",
        "\n",
        "tags = [tag for word, tag in tagged_sentence]\n",
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqW40ZYDNhD0",
        "outputId": "86379b15-1770-4fb8-b616-06fc75f9f516"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NNC', 'NNC', 'VNF', 'VP']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluvation"
      ],
      "metadata": {
        "id": "YuPaqZrSNYok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test data evaluvation\n",
        "\n",
        "import json\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the saved model\n",
        "tagger = SinhalaPOSTagger()\n",
        "tagger.load_model('sinhala_pos_model.json')\n",
        "\n",
        "# Prepare test data\n",
        "test_data = []\n",
        "for i in range(len(test_df)):\n",
        "    try:\n",
        "        test_data.append([[test_df.iloc[i]['word'], test_df.iloc[i]['tag']]])\n",
        "    except KeyError:\n",
        "        print(f\"Skipping row {i} due to missing 'word' or 'tag' column\")\n",
        "\n",
        "true_tags = []\n",
        "predicted_tags = []\n",
        "\n",
        "# Evaluate on the test set\n",
        "for sentence in test_data:\n",
        "    for word, tag in sentence:\n",
        "        if not isinstance(word, str):\n",
        "            continue\n",
        "        tokens = tokenizer.tokenize(word)\n",
        "        tokenized_word = tokens[0] if tokens else word\n",
        "        tagged_words = tagger.tag_sentence([tokenized_word])\n",
        "        if tagged_words:\n",
        "            predicted_word, predicted_tag = tagged_words[0]\n",
        "            true_tags.append(tag)\n",
        "            predicted_tags.append(predicted_tag)\n",
        "\n",
        "print(classification_report(true_tags, predicted_tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XOHivAL2ALy",
        "outputId": "88bfd27e-79d2-4bb7-8fab-f7fe73b1f986"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ABB       0.00      0.00      0.00        16\n",
            "         AUX       0.00      0.00      0.00         7\n",
            "          CC       0.00      0.00      0.00         5\n",
            "          CM       0.00      0.00      0.00        10\n",
            "         DET       0.00      0.00      0.00         9\n",
            "         JCV       0.00      0.00      0.00        48\n",
            "          JJ       0.40      0.01      0.01       326\n",
            "         NCV       0.00      0.00      0.00        58\n",
            "         NDT       0.00      0.00      0.00         7\n",
            "         NIP       1.00      0.12      0.22         8\n",
            "         NNC       0.45      0.99      0.62      2786\n",
            "        NNC‍       0.00      0.00      0.00        28\n",
            "         NNJ       0.00      0.00      0.00       100\n",
            "         NNP       0.57      0.01      0.03      1249\n",
            "         NUM       1.00      0.07      0.13       209\n",
            "         NVB       0.00      0.00      0.00        69\n",
            "        POST       0.50      0.01      0.02        80\n",
            "         PRP       0.00      0.00      0.00        47\n",
            "        PUNC       0.00      0.00      0.00         8\n",
            "         QBE       0.50      0.33      0.40         6\n",
            "         QUE       0.00      0.00      0.00         4\n",
            "          RB       0.00      0.00      0.00        46\n",
            "          RP       0.00      0.00      0.00         6\n",
            "       RRPCV       0.00      0.00      0.00        27\n",
            "         UNK       0.33      0.02      0.04        42\n",
            "         VFM       0.00      0.00      0.00       187\n",
            "         VNF       1.00      0.00      0.01       258\n",
            "         VNN       1.00      0.02      0.03       256\n",
            "          VP       0.57      0.01      0.02       390\n",
            "\n",
            "    accuracy                           0.45      6292\n",
            "   macro avg       0.25      0.06      0.05      6292\n",
            "weighted avg       0.49      0.45      0.29      6292\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking"
      ],
      "metadata": {
        "id": "CK9TcMJ4NHcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grammar_check(pos_tags):\n",
        "    errors = []\n",
        "\n",
        "    # Helper function to get next non-particle tag\n",
        "    def get_next_meaningful_tag(index):\n",
        "        i = index + 1\n",
        "        while i < len(pos_tags):\n",
        "            if pos_tags[i][1] not in ['RP', 'RPCV', 'RPQ']:\n",
        "                return pos_tags[i]\n",
        "            i += 1\n",
        "        return None\n",
        "\n",
        "    # Helper function to get previous non-particle tag\n",
        "    def get_prev_meaningful_tag(index):\n",
        "        i = index - 1\n",
        "        while i >= 0:\n",
        "            if pos_tags[i][1] not in ['RP', 'RPCV', 'RPQ']:\n",
        "                return pos_tags[i]\n",
        "            i -= 1\n",
        "        return None\n",
        "\n",
        "    for i in range(len(pos_tags) - 1):\n",
        "        current_word, current_tag = pos_tags[i]\n",
        "        next_word, next_tag = pos_tags[i + 1]\n",
        "\n",
        "        # Rule 1: Compound Verb Formation\n",
        "        # Check if compound verb components are in correct order\n",
        "        if current_tag in ['NCV', 'ACV', 'RPCV']:\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if next_meaningful and not next_meaningful[1].startswith('V'):\n",
        "                errors.append(f\"Error: Compound verb formation error at '{current_word}'. Expected verb after {current_tag}.\")\n",
        "\n",
        "        # Rule 2: Case Marker Placement\n",
        "        # Case markers should follow nouns\n",
        "        if current_tag == 'CM':\n",
        "            if i == 0 or pos_tags[i-1][1] not in ['NNC', 'NNP', 'PRP']:\n",
        "                errors.append(f\"Error: Case marker '{current_word}' must follow a noun.\")\n",
        "\n",
        "        # Rule 3: Postposition Usage\n",
        "        # Postpositions should follow nouns or pronouns\n",
        "        if current_tag == 'POST':\n",
        "            if i == 0 or pos_tags[i-1][1] not in ['NNC', 'NNP', 'PRP', 'NNJ']:\n",
        "                errors.append(f\"Error: Postposition '{current_word}' must follow a noun or pronoun.\")\n",
        "\n",
        "        # Rule 4: Adjectival Noun Order\n",
        "        # Adjectival nouns should be followed by regular nouns\n",
        "        if current_tag == 'NNJ':\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if not next_meaningful or next_meaningful[1] not in ['NNC', 'NNP']:\n",
        "                errors.append(f\"Error: Adjectival noun '{current_word}' must be followed by a noun.\")\n",
        "\n",
        "        # Rule 5: Determiners Placement\n",
        "        # Determiners should be followed by nouns or adjectival nouns\n",
        "        if current_tag == 'DET':\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if not next_meaningful or next_meaningful[1] not in ['NNC', 'NNP', 'NNJ']:\n",
        "                errors.append(f\"Error: Determiner '{current_word}' must be followed by a noun or adjectival noun.\")\n",
        "\n",
        "        # Rule 6: Verb Finite Position\n",
        "        # Finite verbs should appear at the end of clauses\n",
        "        if current_tag == 'VFM':\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if next_meaningful and next_meaningful[1] not in ['CC', 'PUNC', 'FS']:\n",
        "                errors.append(f\"Error: Finite verb '{current_word}' should appear at end of clause.\")\n",
        "\n",
        "        # Rule 7: Supportive Verb in Compound Verb\n",
        "        # Check proper formation of compound verbs with supportive verbs\n",
        "        if current_tag == 'SVCV':\n",
        "            if i == 0 or not pos_tags[i-1][1].startswith('V'):\n",
        "                errors.append(f\"Error: Supportive verb '{current_word}' must follow a main verb.\")\n",
        "\n",
        "        # Rule 8: Negative Prefix Position\n",
        "        # Negative prefix should be followed by verbs or participles\n",
        "        if current_tag == 'NGP':\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if not next_meaningful or not next_meaningful[1].startswith('V'):\n",
        "                errors.append(f\"Error: Negative prefix '{current_word}' must be followed by a verb or participle.\")\n",
        "\n",
        "        # Rule 9: Particle in Quotation Usage\n",
        "        # Check proper placement of quotation particles\n",
        "        if current_tag == 'RPQ':\n",
        "            if i == 0 or i == len(pos_tags) - 1:\n",
        "                errors.append(f\"Error: Quotation particle '{current_word}' must be between sentence parts.\")\n",
        "\n",
        "        # Rule 10: Sentence Ending\n",
        "        # Check proper sentence endings\n",
        "        if current_tag == 'NNV':\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if next_meaningful and next_meaningful[1] not in ['PUNC', 'FS']:\n",
        "                errors.append(f\"Error: Sentence ending '{current_word}' must be followed by punctuation.\")\n",
        "\n",
        "        # Rule 11: Prefix Usage\n",
        "        # Check proper placement of prefixes\n",
        "        if current_tag == 'PRF':\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if not next_meaningful:\n",
        "                errors.append(f\"Error: Prefix '{current_word}' must be followed by a word.\")\n",
        "\n",
        "        # Rule 12: Adverbial Suffix Position\n",
        "        # Adverbial suffix should follow adjectives or adverbs\n",
        "        if current_tag == 'AVS':\n",
        "            prev_meaningful = get_prev_meaningful_tag(i)\n",
        "            if not prev_meaningful or prev_meaningful[1] not in ['JJ', 'RB']:\n",
        "                errors.append(f\"Error: Adverbial suffix '{current_word}' must follow an adjective or adverb.\")\n",
        "\n",
        "        # Rule 13: Verbal Suffix Position\n",
        "        # Verbal suffix should appear in place of verbs\n",
        "        if current_tag == 'VSX':\n",
        "            prev_meaningful = get_prev_meaningful_tag(i)\n",
        "            if prev_meaningful and prev_meaningful[1].startswith('V'):\n",
        "                errors.append(f\"Error: Verbal suffix '{current_word}' cannot follow a verb.\")\n",
        "\n",
        "        # Rule 14: Proper Noun Compound Formation\n",
        "        # All parts of compound proper nouns should be tagged as NNP except nipātha\n",
        "        if current_tag == 'NNP':\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if next_meaningful and next_meaningful[1] not in ['NNP', 'POST', 'CM', 'PUNC', 'FS']:\n",
        "                errors.append(f\"Error: Compound proper noun parts should all be tagged as NNP.\")\n",
        "\n",
        "        # Rule 15: Question Based Pronoun Usage\n",
        "        # Check proper formation of questions using QBE\n",
        "        if current_tag == 'QBE':\n",
        "            if i == len(pos_tags) - 1 or pos_tags[-1][1] not in ['FS', 'PUNC']:\n",
        "                errors.append(f\"Error: Question based pronoun '{current_word}' should form a complete question.\")\n",
        "\n",
        "        # Rule 16: Conjunction Usage\n",
        "        # Check proper placement of conjunctions\n",
        "        if current_tag == 'CC':\n",
        "            prev_meaningful = get_prev_meaningful_tag(i)\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if not prev_meaningful or not next_meaningful:\n",
        "                errors.append(f\"Error: Conjunction '{current_word}' must connect two elements.\")\n",
        "\n",
        "        # Rule 17: Base Form Check for Adjectival Nouns\n",
        "        # Adjectival Nouns should be in base form (plural for countable nouns)\n",
        "        if current_tag == 'NNJ':\n",
        "            # This would need a dictionary of base forms to check against\n",
        "            # For now, we can add a placeholder check\n",
        "            pass\n",
        "\n",
        "        # Rule 18: Nipathana Usage\n",
        "        # Check proper usage of Nipathana words\n",
        "        if current_tag == 'NIP':\n",
        "            next_meaningful = get_next_meaningful_tag(i)\n",
        "            if next_meaningful and next_meaningful[1] not in ['NNC', 'NNP', 'PRP', 'POST']:\n",
        "                errors.append(f\"Error: Improper usage of Nipathana '{current_word}'.\")\n",
        "\n",
        "        # Rule 19: Interjection Position\n",
        "        # Interjections typically appear at the start of expressions\n",
        "        if current_tag == 'UH':\n",
        "            prev_meaningful = get_prev_meaningful_tag(i)\n",
        "            if prev_meaningful and prev_meaningful[1] not in ['PUNC', 'FS']:\n",
        "                errors.append(f\"Error: Interjection '{current_word}' should appear at the start of an expression.\")\n",
        "\n",
        "        # Rule 20: Deterministic Pronoun Formation\n",
        "        # Check proper formation of deterministic pronouns\n",
        "        if current_tag == 'NDT':\n",
        "            prev_meaningful = get_prev_meaningful_tag(i)\n",
        "            if prev_meaningful and prev_meaningful[1] == 'DET':\n",
        "                errors.append(f\"Error: Redundant determiner before deterministic pronoun '{current_word}'.\")\n",
        "\n",
        "    return errors"
      ],
      "metadata": {
        "id": "5eGoTssLVxzT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = grammar_check(tagged_sentence)\n",
        "print(errors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7TlASpQOLtE",
        "outputId": "f8b9f16d-79b7-4814-c550-ac6dd7df9578"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correction"
      ],
      "metadata": {
        "id": "7ikpIcNGXXy1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYfj9UQbXZ_7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}